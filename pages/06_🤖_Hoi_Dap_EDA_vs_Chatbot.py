### --- 0. IMPORT CÃC THÆ¯ VIá»†N Cáº¦N THIáº¾T ---
import os 
import streamlit as st 

# ThÆ° viá»‡n há»— trá»£ load biáº¿n mÃ´i trÆ°á»ng tá»« file .env
from dotenv import load_dotenv

# ThÆ° viá»‡n há»— trá»£ load dá»¯ liá»‡u tá»« file Notebook .ipynb
from langchain_community.document_loaders import NotebookLoader

# ThÆ° viá»‡n chia vÄƒn báº£n lá»›n thÃ nh cÃ¡c phÃ¢n Ä‘oáº¡n nhá» hÆ¡n Ä‘á»ƒ xá»­ lÃ½ hiá»‡u quáº£ hÆ¡n.
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ThÆ° viá»‡n há»— trá»£ táº¡o embeddings sá»­ dá»¥ng mÃ´ hÃ¬nh AITeamVN/Vietnamese_Embedding trÃªn HuggingFace.
from langchain_huggingface import HuggingFaceEmbeddings

# ThÆ° viá»‡n há»— trá»£ lÆ°u trá»¯ vÃ  truy xuáº¥t vector database sá»­ dá»¥ng FAISS.
from langchain_community.vectorstores import FAISS 

# ThÆ° viá»‡n há»— trá»£ táº¡o máº«u prompt cho mÃ´ hÃ¬nh ngÃ´n ngá»¯
from langchain.prompts import PromptTemplate 

# ThÆ° viá»‡n há»— trá»£ táº¡o chuá»—i há»i Ä‘Ã¡p (question answering chain)
from langchain.chains.question_answering import load_qa_chain

# ThÆ° viá»‡n Google Generative AI Ä‘á»ƒ gá»i API
import google.generativeai as genai 
# ThÆ° viá»‡n langchain Ä‘á»ƒ tÃ­ch há»£p vá»›i Google Generative AI
from langchain_google_genai import ChatGoogleGenerativeAI

# ThÆ° viá»‡n há»— trá»£ truy xuáº¥t Ä‘a truy váº¥n (Multi-Query Retriever)
from langchain.retrievers.multi_query import MultiQueryRetriever

# ThÆ° viá»‡n há»— trá»£ lÆ°u trá»¯ lá»‹ch sá»­ há»™i thoáº¡i (conversation memory)
from langchain.memory import ConversationBufferMemory

# ThÆ° viá»‡n há»— trá»£ táº¡o chuá»—i há»i Ä‘Ã¡p vá»›i kháº£ nÄƒng truy xuáº¥t thÃ´ng tin vÃ  sá»­ dá»¥ng memory
from langchain.chains import ConversationalRetrievalChain

# Sá»­ dá»¥ng cÃ¡c hÃ m dá»± Ä‘oÃ¡n trong utils.py
from utils import load_ols_model, predict_spending

# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n lÆ°u trá»¯ vector database
from config import VECTOR_STORE_PATH

# --- 1. CÃ€I Äáº¶T vs KHá»I Táº O MÃ”I TRÆ¯á»œNG LÃ€M VIá»†C Vá»šI GOOGLE API KEY ---
# load biáº¿n mÃ´i trá»«á»ng tá»« file .env
load_dotenv()

# Láº¥y GOOGLE_API_KEY tá»« biáº¿n mÃ´i trÆ°á»ng
api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    st.error("KhÃ´ng tÃ¬m tháº¥y key")
    st.stop()
    # Náº¿u khÃ´ng cÃ³ API key thÃ¬ dá»«ng chÆ°Æ¡ng trÃ¬nh láº¡i

# CÃ i Ä‘áº·t API key cho Google Genai, khá»Ÿi táº¡o mÃ´i trÆ°á»ng
genai.configure(api_key = api_key)


# --- 2. HÃ€M XÃ‚Y Dá»°NG KHO Dá»® LIá»†U VECTOR Káº¾T QUáº¢ EDA Cá»¦A DATASET  ---
@st.cache_resource
def build_EDA_results_store():
    try:
        # BÆ°á»›c 1: Load tá»« .ipynb, bao gá»“m cáº£ outputs     
        loader = NotebookLoader(
            path='./Customer_Segmentation_EDA.ipynb',
            include_outputs=True,  # Load outputs tá»« code cells (text, HTML sáº½ thÃ nh string)
            max_output_length=3000,  # Giá»›i háº¡n Ä‘á»™ dÃ i output Ä‘á»ƒ trÃ¡nh quÃ¡ lá»›n
            remove_newline=True  # XÃ³a newline thá»«a cho clean text
        )
        # Load toÃ n bá»™ notebook thÃ nh Documents (source + outputs náº¿u cÃ³)
        documents = loader.load() 

        # BÆ°á»›c 2: Split vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n nhá» hÆ¡n
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,  # KÃ­ch thÆ°á»›c má»—i chunk
            chunk_overlap=300,  # Overlap Ä‘á»ƒ giá»¯ ngá»¯ cáº£nh
        )
        split_docs = text_splitter.split_documents(documents)
        
        # BÆ°á»›c 3: Vector hÃ³a vÃ  lÆ°u vÃ o FAISS (dÃ¹ng AITeamVN/Vietnamese_Embedding trÃªn HuggingFace)
        ## Táº¡o embeddings sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯
        embed_model = HuggingFaceEmbeddings(
            model_name="AITeamVN/Vietnamese_Embedding",
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )  
        
        ## Táº¡o vector store tá»« cÃ¡c document Ä‘Ã£ chia nhá»
        vector_store = FAISS.from_documents(split_docs, embed_model)
        ## Hiá»ƒn thá»‹ thÃ´ng tin sá»‘ Ä‘oáº¡n Ä‘Ã£ load vÃ  vector hÃ³a
        st.info(f"ÄÃ£ load vÃ  vector hÃ³a {len(split_docs)} Ä‘oáº¡n insight (dÃ¹ng AITeamVN/Vietnamese_Embedding Embeddings).")
        
        ## LÆ°u vector store vÃ o local
        vector_store.save_local("faiss_index")  # LÆ°u vÃ o thÆ° má»¥c faiss_index 
  
        st.success("Dá»¯ liá»‡u Kinh doanh Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n tÃ­ch xong, sáºµn sÃ ng Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i")     
    except Exception as e:
        st.error(f"Lá»—i lÆ°u vector database: {str(e)}")
 
# # HÃ m kiá»ƒm tra VectorDB Ä‘Ã£ tá»“n táº¡i chÆ°a
def check_EDA_results_store(): 
    return True if os.path.exists(VECTOR_STORE_PATH) else False

# --- 3. HÃ€M Táº O CHUá»–I Há»I ÄÃP CONVERSATIONAL Vá»šI MEMORY ---
def get_conversational_chain(retriever, memory):    
    prompt_template = """
    Báº¡n lÃ  má»™t Trá»£ lÃ½ AI chuyÃªn phÃ¢n tÃ­ch dá»¯ liá»‡u. 
    Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i chi tiáº¿t cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng Dá»°A HOÃ€N TOÃ€N VÃ€O Ná»˜I DUNG (Context) Ä‘Æ°á»£c cung cáº¥p, káº¿t há»£p vá»›i lá»‹ch sá»­ chat náº¿u cáº§n Ä‘á»ƒ infer thÃªm.
    Ná»™i dung nÃ y Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« cÃ¡c káº¿t luáº­n (insight) trong má»™t file Jupyter Notebook "Customer_Segmentation_EDA.ipynb".
    
    HÃ£y Ä‘á»c ká»¹ Context vÃ  lá»‹ch sá»­ chat, tráº£ lá»i rÃµ rÃ ng, sÃºc tÃ­ch. Náº¿u cÃ¢u há»i follow-up (nhÆ° 'liá»‡t kÃª Ä‘áº§y Ä‘á»§'), hÃ£y liÃªn káº¿t vá»›i query trÆ°á»›c.
    
    QUAN TRá»ŒNG: 
    1- Náº¿u cÃ¢u tráº£ lá»i khÃ´ng cÃ³ trong Context, hÃ£y thá»­ paraphrase hoáº·c dÃ¹ng history Ä‘á»ƒ tÃ¬m thÃªm. Náº¿u váº«n khÃ´ng, nÃ³i: "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin chÃ­nh xÃ¡c, nhÆ°ng dá»±a trÃªn insight gáº§n nháº¥t: [giáº£i thÃ­ch ngáº¯n]". KhÃ´ng Ä‘Æ°á»£c bá»‹a thÃ´ng tin.
    2- CÃ¢u tráº£ lá»i Ä‘Æ°a ra pháº£i báº±ng tiáº¿ng Viá»‡t.
    3- CÃ¢u tráº£ lá»i cáº§n Ä‘Ãºng ngá»¯ cáº£nh lÃ  Ä‘ang trong cuá»™c trÃ² chuyá»‡n, chá»© khÃ´ng Ä‘Æ°á»£c Ä‘Æ°a ra tá»« ngá»¯ dÃ¹ng trong ngá»¯ cáº£nh Ä‘ang Ä‘á»c tÃ i liá»‡u. VÃ­ dá»¥, khÃ´ng nÃ³i kiá»ƒu "xem láº¡i cÃ¡c pháº§n trÆ°á»›c cá»§a phÃ¢n tÃ­ch Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t", "Ä‘Æ°á»£c trÃ­ch tá»« pháº§n tá»•ng káº¿t C",...
    Context:
    {context}
    
    Question:
    {question}
    
    Answer (tráº£ lá»i báº±ng tiáº¿ng Viá»‡t):
    """    
    try:
        # Khá»Ÿi táº¡o mÃ´ hÃ¬nh LLM cá»§a Google Gemini
        model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.5)
        
        # Äá»‹nh nghÄ©a Prompt Template
        qa_prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])        
        
        # Táº¡o chuá»—i conversational retrieval vá»›i memory
        chain = ConversationalRetrievalChain.from_llm(
            llm=model,
            retriever=retriever,
            memory=memory,
            combine_docs_chain_kwargs={"prompt": qa_prompt},
            return_source_documents=False  # KhÃ´ng cáº§n tráº£ vá» source docs
        )
                
        return chain            
    except Exception as e:
        st.error(f"Lá»—i táº¡o chuá»—i há»i Ä‘Ã¡p: {str(e)}")
    return None

# --- HÃ€M DETECT USER INTENT Sá»¬ Dá»¤NG GEMINI ---
def detect_intent(user_question):
    try:
        model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.3)
        prompt = f"""
        PhÃ¢n loáº¡i intent cá»§a cÃ¢u há»i sau: '{user_question}'
        Náº¿u intent lÃ  'dá»± Ä‘oÃ¡n chi tiÃªu' hoáº·c liÃªn quan Ä‘áº¿n dá»± Ä‘oÃ¡n chi tiÃªu khÃ¡ch hÃ ng, tráº£ vá» 'prediction'.
        Náº¿u khÃ´ng, tráº£ vá» 'general'.
        Chá»‰ tráº£ vá» 'prediction' hoáº·c 'general', khÃ´ng giáº£i thÃ­ch.
        """
        response = model.invoke(prompt)
        intent = response.content.strip().lower()
        return intent == 'prediction'
    except Exception as e:
        st.error(f"Lá»—i detect intent: {str(e)}")
        return False

# --- 4. HÃ€M NHáº¬N VÃ” CÃ‚U Há»I Cá»¦A USER VÃ€ Xá»¬ LÃ, TRáº¢ Vá»€ CÃ‚U TRáº¢ Lá»œI
def user_input(user_question):
    try:        
        #*** KIáº¾M TRA XEM CHáº¾ Äá»˜ Dá»° ÄOÃN ÄÃƒ ÄÆ¯á»¢C KÃCH HOáº T CHÆ¯A ***
        if st.session_state.get('prediction_mode', False):
            current_step = st.session_state.get('prediction_step', 'income')
            
            if current_step == 'income':
                try:
                    income = float(user_question)
                    if income < 0:
                        raise ValueError
                    st.session_state.prediction_data['income'] = income
                    st.session_state.prediction_step = 'total_children'
                    return "Vui lÃ²ng cho biáº¿t tá»•ng sá»‘ con (sá»‘ nguyÃªn, vÃ­ dá»¥ 0,1,2,...):"
                except ValueError:
                    return "Thu nháº­p pháº£i lÃ  sá»‘ thá»±c khÃ´ng Ã¢m. Vui lÃ²ng nháº­p láº¡i."
            
            elif current_step == 'total_children':
                try:
                    total_children = int(user_question)
                    if total_children < 0:
                        raise ValueError
                    st.session_state.prediction_data['total_children'] = total_children
                    st.session_state.prediction_step = 'customer_tenure'
                    return "Vui lÃ²ng cho biáº¿t thÃ¢m niÃªn (sá»‘ ngÃ y, vÃ­ dá»¥ 365):"
                except ValueError:
                    return "Tá»•ng sá»‘ con pháº£i lÃ  sá»‘ nguyÃªn khÃ´ng Ã¢m. Vui lÃ²ng nháº­p láº¡i."
            
            elif current_step == 'customer_tenure':
                try:
                    customer_tenure = int(user_question)
                    if customer_tenure < 0:
                        raise ValueError
                    st.session_state.prediction_data['customer_tenure'] = customer_tenure
                    
                    # Dá»± Ä‘oÃ¡n (gá»i tá»« utils.py)
                    data = st.session_state.prediction_data
                    result = predict_spending(data['income'], data['total_children'], data['customer_tenure'])
                    
                    # Reset tráº¡ng thÃ¡i
                    st.session_state.prediction_mode = False
                    del st.session_state.prediction_step
                    del st.session_state.prediction_data
                    
                    if result is not None:
                        return f"Káº¿t quáº£ dá»± Ä‘oÃ¡n chi tiÃªu: {result} USD."
                    else:
                        return "Lá»—i trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n. Vui lÃ²ng thá»­ láº¡i."
                except ValueError:
                    return "ThÃ¢m niÃªn pháº£i lÃ  sá»‘ nguyÃªn khÃ´ng Ã¢m. Vui lÃ²ng nháº­p láº¡i."
        
        #*** Gá»ŒI HÃ€M DETECT INTENT Äá»‚ XEM CÃ“ PHáº¢I Dá»° ÄOÃN CHI TIÃŠU KHÃ”NG ***
        # (Láº½ ra pháº§n nÃ y sáº½ Ä‘áº·t trÆ°á»›c "KIáº¾M TRA XEM CHáº¾ Äá»˜ Dá»° ÄOÃN ÄÃƒ ÄÆ¯á»¢C KÃCH HOáº T CHÆ¯A"
        # Tuy nhiÃªn Ä‘á»ƒ trÃ¡nh viá»‡c láº·p láº¡i detect intent nhiá»u láº§n khi user Ä‘ang nháº­p dá»¯ liá»‡u (income, children, tenure))
        if detect_intent(user_question): 
        # Náº¿u Ä‘Ãºng lÃ  intent dá»± Ä‘oÃ¡n chi tiÃªu
            ## Load mÃ´ hÃ¬nh OLS chá»‰ khi cáº§n (láº§n Ä‘áº§u tiÃªn) vÃ  cache qua session_state
            if not st.session_state.get('ols_loaded', False):
                with st.spinner("Äang táº£i mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n (chá»‰ láº§n Ä‘áº§u)..."):
                    ols_results, scaler_ols, feature_cols_ols = load_ols_model()
                    if ols_results is None:
                        return "Lá»—i táº£i mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n. Vui lÃ²ng thá»­ láº¡i sau."
                    st.session_state.ols_results = ols_results
                    st.session_state.scaler_ols = scaler_ols
                    st.session_state.feature_cols_ols = feature_cols_ols
                    st.session_state.ols_loaded = True
            
            ## Khá»Ÿi Ä‘á»™ng cháº¿ Ä‘á»™ dá»± Ä‘oÃ¡n
            st.session_state.prediction_mode = True
            st.session_state.prediction_step = 'income'
            st.session_state.prediction_data = {}
            return "Äá»ƒ dá»± Ä‘oÃ¡n chi tiÃªu ngÆ°á»i dÃ¹ng, vui lÃ²ng cung cáº¥p thu nháº­p (sá»‘ thá»±c khÃ´ng Ã¢m, USD):"       
        
        
        #*** CHáº¾ Äá»˜ Há»I ÄÃP BÃŒNH THÆ¯á»œNG Náº¾U KHÃ”NG PHáº¢I PREDICTION***
        ## Load embed_model vÃ  vector_store chá»‰ khi cáº§n, cache trong session_state
        if 'embed_model' not in st.session_state:
            with st.spinner("Äang táº£i embedding model láº§n Ä‘áº§u (sáº½ nhanh hÆ¡n láº§n sau)..."):
                st.session_state.embed_model = HuggingFaceEmbeddings(
                    model_name="AITeamVN/Vietnamese_Embedding",
                    model_kwargs={"device": "cpu"},
                    encode_kwargs={"normalize_embeddings": True}
                )        
        embed_model = st.session_state.embed_model
        
        ## Load vector_store tá»« session_state náº¿u Ä‘Ã£ cÃ³, náº¿u chÆ°a thÃ¬ load tá»« local
        if not check_EDA_results_store():
            st.error("EDA Results Vector store khÃ´ng tá»“n táº¡i. HÃ£y táº¡o trÆ°á»›c!")        
       
        if 'vector_store' not in st.session_state:
            with st.spinner("Äang táº£i vector store láº§n Ä‘áº§u..."):
                st.session_state.vector_store = FAISS.load_local(
                    VECTOR_STORE_PATH, embed_model, allow_dangerous_deserialization=True
                )            
        vector_store = st.session_state.vector_store

        # Láº¥y retriever tá»« vector store (tÃ¬m 7 káº¿t quáº£ liÃªn quan nháº¥t)
        base_retriever = vector_store.as_retriever(search_kwargs={"k": 7})
        
        # Táº¡o MultiQueryRetriever Ä‘á»ƒ LLM tá»± generate 3-5 variant queries
        # Khá»Ÿi táº¡o LLM cho MultiQuery (dÃ¹ng cÃ¹ng Gemini model)
        llm_for_multi = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.5)
        
        # Custom prompt Ä‘á»ƒ generate 3-5 variants báº±ng tiáº¿ng Viá»‡t
        multi_prompt = PromptTemplate.from_template(
            "You are an AI assistant. Suggest 3 to 5 alternative questions in Vietnamese that capture different perspectives on the user question: {question}. Output each on a new line."
        )
        
        multi_retriever = MultiQueryRetriever.from_llm(
            retriever=base_retriever,  # Wrap retriever hiá»‡n táº¡i
            llm=llm_for_multi,  # LLM Ä‘á»ƒ generate variants
            prompt=multi_prompt  # Sá»­ dá»¥ng custom prompt
        )
        
        # Sá»­ dá»¥ng multi_retriever thay vÃ¬ base_retriever
        retriever = multi_retriever
        
        # Láº¥y memory tá»« session_state (sáº½ Ä‘Æ°á»£c khá»Ÿi táº¡o á»Ÿ pháº§n chÃ­nh)
        memory = st.session_state.memory
        
        # Láº¥y chuá»—i há»i Ä‘Ã¡p conversational
        qa_chain = get_conversational_chain(retriever, memory)
        
        ## náº¿u khÃ´ng cÃ³ chain thÃ¬ tráº£ vá» rá»—ng
        if not qa_chain:
            return
        
        # Táº¡o cÃ¢u tráº£ lá»i dá»±a trÃªn cÃ¢u há»i vÃ  lá»‹ch sá»­ chat (memory sáº½ tá»± xá»­ lÃ½)
        response = qa_chain({"question": user_question})
       
        return response['answer']
    
    except Exception as e:
        st.error(f"Lá»—i xá»­ lÃ½ cÃ¢u há»i: {str(e)}")  
    return None
    
# --- 5. GIAO DIá»†N STREAMLIT CHO á»¨NG Dá»¤NG CHATBOT Há»I ÄÃP Vá»€ Káº¾T QUáº¢ EDA ---

st.set_page_config(page_title="Chatbot Giáº£i thÃ­ch Insight EDA", layout="wide")

st.title("ğŸ¤– Chatbot Giáº£i thÃ­ch Insight EDA vá» ChÃ¢n dung khÃ¡ch hÃ ng")
try: 
    # Kiá»ƒm tra kho dá»¯ liá»‡u "EDA Results Vector Database" Ä‘Ã£ tá»“n táº¡i chÆ°a   
    is_has_EDA_results_store = check_EDA_results_store()    
    if is_has_EDA_results_store:
        st.success("âœ… EDA Results Vector Database Ä‘Ã£ táº¡o thÃ nh cÃ´ng. Báº¡n cÃ³ thá»ƒ há»i Ä‘Ã¡p ngay bÃ¢y giá»!")
    else:        
        # NÃºt Ä‘á»ƒ xÃ¢y dá»±ng kho dá»¯ liá»‡u vector tá»« káº¿t quáº£ EDA
        st.error("âš ï¸ ChÆ°a tÃ¬m tháº¥y EDA Results Vector Database. Vui lÃ²ng báº¥m nÃºt bÃªn dÆ°á»›i Ä‘á»ƒ táº¡o kho dá»¯ liá»‡u 'EDA RESULTS VECTOR DATABASE' tá»« file 'Customer_Segmentation_EDA.ipynb' trÆ°á»›c khi há»i Ä‘Ã¡p.")
        if st.button("ğŸ‘‰ Báº®T Äáº¦U Táº O 'EDA RESULTS VECTOR DATABASE'"):
            build_EDA_results_store()           
    
    # Khá»Ÿi táº¡o lá»‹ch sá»­ chat
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Xin chÃ o! TÃ´i lÃ  AI Assistant phÃ¢n tÃ­ch dá»¯ liá»‡u. HÃ£y há»i tÃ´i báº¥t cá»© Ä‘iá»u gÃ¬ vá» táº­p dá»¯ liá»‡u \"marketing_data_with_missing_values.csv\" vÃ  tÃ´i sáº½ tráº£ lá»i dá»±a trÃªn cÃ¡c káº¿t luáº­n (insight) Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n tÃ­ch. Báº¡n cÅ©ng cÃ³ thá»ƒ há»i vá» dá»± Ä‘oÃ¡n chi tiÃªu ngÆ°á»i dÃ¹ng Ä‘á»ƒ kÃ­ch hoáº¡t tÃ­nh nÄƒng dá»± Ä‘oÃ¡n."}]
    
    # Khá»Ÿi táº¡o memory cho conversational chain
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
    # Khá»Ÿi táº¡o tráº¡ng thÃ¡i cho prediction vs ols_loaded náº¿u chÆ°a cÃ³
    if "prediction_mode" not in st.session_state:
        st.session_state.prediction_mode = False
    if "prediction_data" not in st.session_state:
        st.session_state.prediction_data = {}
    if "prediction_step" not in st.session_state:
        st.session_state.prediction_step = 'income'
    if "ols_loaded" not in st.session_state:
        st.session_state.ols_loaded = False
        
    # Hiá»ƒn thá»‹ cÃ¡c tin nháº¯n cÅ©
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Nháº­n input tá»« ngÆ°á»i dÃ¹ng
    if prompt := st.chat_input("Nháº­p cÃ¢u há»i cá»§a báº¡n vá» káº¿t quáº£ EDA á»Ÿ Ä‘Ã¢y: VÃ­ dá»¥, 'CÃ³ cÃ¡c phÃ¢n khÃºc khÃ¡ch hÃ ng nÃ o'? Hoáº·c há»i vá» dá»± Ä‘oÃ¡n chi tiÃªu"):
        
        # 1. ThÃªm cÃ¢u há»i ngÆ°á»i dÃ¹ng vÃ o lá»‹ch sá»­ vÃ  hiá»ƒn thá»‹ ngay
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
            
        # 2. Xá»­ lÃ½ cÃ¢u há»i vÃ  hiá»ƒn thá»‹ cÃ¢u tráº£ lá»i cá»§a AI
        assistant_response = ""         
        with st.chat_message("assistant"):
            with st.spinner("Chatbot Ä‘ang suy nghÄ©..."):
                if is_has_EDA_results_store:
                    # Gá»i hÃ m xá»­ lÃ½ cÃ¢u há»i cá»§a user, vÃ  nháº­n cÃ¢u tráº£ lá»i cá»§a bot
                    assistant_response = user_input(prompt)
                    
                    # Hiá»ƒn thá»‹ cÃ¢u tráº£ lá»i cá»§a bot
                    st.markdown(assistant_response)                
                else:
                    st.warning("Vui lÃ²ng táº¡o 'EDA Results Vector Database' trÆ°á»›c khi há»i Ä‘Ã¡p!")
                
        # 3. ThÃªm cÃ¢u tráº£ lá»i cá»§a bot vÃ o lá»‹ch sá»­ chat
        if assistant_response:
            st.session_state.messages.append({"role": "assistant", "content": assistant_response})

except Exception as e:
    st.error(f"ÄÃ£ xáº£y ra lá»—i: {e}")
    st.error("Lá»–I: Vui lÃ²ng kiá»ƒm tra láº¡i GOOGLE_API_KEY cá»§a báº¡n vÃ  Ä‘áº£m báº£o file 'Customer_Segmentation_EDA.ipynb' á»Ÿ cÃ¹ng thÆ° má»¥c.")
